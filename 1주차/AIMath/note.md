* # <벡터> 

---
* # <행렬>
  * 행렬곱
    * 행과 열의 조건을 맞춰야 연산가능함.
  * 역행렬
    * 곱의 연산을 했을 때 항등행렬이 나온다.
  * 유사 역행렬
    * 무어펜로스(Moore-Penrose) 역행렬
      * ![무어펜로즈.png](%EB%AC%B4%EC%96%B4%ED%8E%9C%EB%A1%9C%EC%A6%88.png)
      * `np.linalg.pinv`
      * 행과 열의 크기에 따라 순서가 달라지니 유의 할 것.
    * 연립방정식, 선형회귀 분석에 응용됨.
  * `생각`
    * 왜 행렬을 배우는가? 차원이동, 연산
  * 문제
    * 사이킷런의 선형회귀 모델과 무어펜로즈의 선형회귀를 직접 구현해보시오.

#### 딥러닝을 제대로 이해하기 위해선 '선형대수학'을 수준 높게 학습해야 한다

---
# <경사하강법> 
* 미분
  * 접선의 기울기
  * sysmpy.diff -> 미분계산 가능
  * **<어느 차원에서든>** 그 점에서 증가하는가, 감소하는가를 알 수 있다.
  * 어느 차원으로 확장한다는 것은 변수를 스칼라가 아닌 벡터를 사용한다는 것.
* 경사하강법
  * 감소하는 방향으로 쭈욱 가다 보면 언젠가 평지를(극소값)을 만날 것이야 => 경사하강법
  * 컴퓨터에서 미분값이 0 인 곳을 찾기란 쉽지 않으므로 아주 작은 값(오메가) 보다 더 미분 값이 작으면 종료하면 되시것다.
  * 학습률 -> 얼마나 큰 보폭으로 갈것인가? (하이퍼 파라메터)
  * 모든 데이터 사용
    * 성능이 확률적 경사 하강법에 비해 좋지 않고, 하드웨어에 부담이 간다.
* 변수가 벡터인경우엔
  * 편미분을 사용한다.
* 그레디언트 벡터
  * 벡터의 변수 별로 편미분을 계산한 함수
  * ![gradient.png](gradient.png)
  * 그레디언트 벡터에 - 를 붙여서 이동하게 되면 **가장 빨리** 극소값을 향해 가게 된다.
  * 그레디언트 벡터의 `norm` 값을 구해서 일정 값보다 작게 되면 학습을 종료하는 방식으로 극소값을 찾으면 된다. (계속 내려가다가 더 이상 변화가 없는 것 같아. -> 일단 멈춰!)
# 선형회귀 모델에서 경사하강법 적용해보기
* 위에서 무어 펜로즈 행렬을 통해서 선형 모델을 어떻게 찾아야 할 지 알았다.
* 우리가 해야 할것은 무어펜로즈 행렬을 곱해서 나온 우변 식을 경사하강법을 하지 않고 !! 오직 경사 하강법을 적용시켜 극소값이 되는 지점을 찾는 것이다.
* #####  this is funny ! 
  * 직접 계산해보자.
* 학습률과 학습횟수를 적절하게 선택했을 때만 수렴을 보장할 수 있다.
* 비선형회귀의 경우, 볼록하지 않기 때문에 수렴을 보장할 수 없다.

# 확률적 경사하강법
*   데이터를 일부 사용해서 경사하강법을 적용하는 것.
  * 데이터 하나만 사용

# 미니배치 확률 경사하강법 (일반적)
  * 데이터 여러개 사용 
  * SGD 가 경사하강법보다 낫다는 것이 실증적으로 검증되었다.
  * 데이터를 일부로 사용하기 때문에 목적식이 매번 달라짐.
  * 학습률, 미니배치사이즈 고려해야 함.


---
# 5. 딥러닝 학습방법 
* 이제 비선형모델인 신경망을 도전해보자.
* 소프트맥스 
  * 모델의 출력을 확률로 해석 => 확률 벡터로 변환
    * 가장 큰 값이 예측값이 되는 것임.
* 신경망은 **선형모델과 활성함수를 합성한 함수**다. [MLP]
  * 잠재벡터들의 누적 공간에 활성화 함수를 적용한다.
* 활성함수
  * R 위에 정의된 비선형 함수
  * 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다.
  * ex) Relu, tanh, sigmoid
* 순전파
  * 주어진 신경망 계산을 하는 것
* 왜 층을 여러개 쌓는가?
  * 수학적으로 임의의 연속함수를 근사할 수 있다는 것이 증명되어 있다.
  * '세계'를 표현할 수 있다.
* 역전파
  * 경사하강법을 이용해서 가중치를 업데이트 하는 것.
  * 위층부터 저층으로 그레디어트 벡터를 전달해야 함(연쇄법칙)
  * 메모리에 저장해서 사용해야 함.
  * tensorflow, pytorch 에는 자동 구현되어 있음.


---
# 6. 확률론
* 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다.
  * 확률론이 중요한 이유
  * 사례
    * 분류 문제
      * 예측 오차의 분산을 최소화
    * 교차 엔트로피 문제
      * 모델 예측의 불확실성을 최소화
* 확률 분포는 데이터의 초상화
* 확률은 면적이다.
* 확률변수
  * 이산 확률 변수
    * 이산 확률을 급수(시그마)
  * 연속 확률 변수
    * 연속 확률을 적분
* 목표
  * 데이터의(표본) 확률 분포를 가지고 -> 실제 (모집단)의 확률 분포를 추정한다.
* 주변확률분포
* 통계값
  * 기댓값
  * 분산
  * 첨도
  * 공분산
  * 조건부확률
* **몬테카를로 샘플링**
  * 데이터(샘플링)를 통해 기댓값을 계산하는 방법
  * 대수의 법칙을 통해 수렴성을 보장한다.
* 


---
# 7. 통계학
- 모수
  - 통계적 모델링 : 확률분포를 추정하는 것.
  - 모수 추정
    - 모수적 방법론
      - 특정 확률 분포를 가정 후 모수를 추정하는 것.
    - 비 모수적 방법론
      - 가정 없이.
- 데이터 관찰 -> 확률 분포 가정
  - 베르누이 분포
  - 카테고리 분포
  - 베타 분포
  - 감마분포, 로그정규분포
  - 정규분포, 라플라스분포
- 데이터 관찰 -> 확률 분포 가정 -> 평균, 분산 추정 -> 모수 집단 추청
- 중심극한정리
  - 표뵨 평균의 표집분포는 N이 커질수록 정규분포를 따른다.
- 표집분포
  - 통계량의 확률분포
- 최대가능도 추정법
  - **가장 가능성이 높은 모수를 추정하는 방법**
  - x 에서 가장 높은 모수 집단 (가능도와 관점의 차이)
  - 기계학습에서 많이 쓰인다.
  - 가능도
    - **모수 쌔타 분포에서 데이터 x 를 발견할 가능성.**
    - 계산상 `로그가능도` 사용
- 정규분포에서 최대 가능도 추정
- 카테고리 분포에서 최대 가능도 추정
- 딥러닝에서 최대가능도 추정법
  - 소프트맥스 벡터 => 카테고리 분포
  - 두 개의 확률분포의 손실함부를 학습시키기
    - 쿨백-라이블러 발산을 최소화
    - 분류 문제에서 정답레이블을 P, 모델 예측을 Q 라 두면 최대가능도 추정법은 쿨백-라이블러 발산을 최소화하는 것과 같음
  
> 최대가능도 추정은 굉장히 많이 쓰이므로 잘 기억하시라.


- 확률
- [중요] 최대 가능도 추정법
  - 정답 확률분포 - 모델 추정 확률분포 손실함수 구하기 => 학습
- 확률과 가능도의 차이
- 가능도를 사용하는 이유? 이점 ? 


---
# 8. 베이즈 통계학


---
# 9. CNN

---
# 10. RNN

---
# 11. Pytorch



---
# 생각해 볼 문제
- 
- 
- 